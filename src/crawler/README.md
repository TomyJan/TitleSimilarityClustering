# 数据爬取模块

## 模块说明

数据爬取模块负责从指定来源获取论文标题数据，并将其保存为标准格式。

## 主要功能

1. 数据爬取：
   - 支持按年份爬取论文标题
   - 自动清洗和格式化数据
   - 支持断点续爬
   
2. 数据验证：
   - 检查数据完整性
   - 验证字段格式
   - 去除重复数据

3. 错误处理：
   - 自动重试失败的请求
   - 记录爬取异常
   - 保存爬取进度

## 实现细节

1. 请求处理：
   - 使用 requests 库发送 HTTP 请求
   - 添加随机延时避免被封
   - 支持代理设置

2. 数据解析：
   - 使用 BeautifulSoup 解析页面
   - 提取必要字段信息
   - 清理HTML标签和特殊字符

3. 数据存储：
   - 使用 CSV 格式保存数据
   - 按年份分文件存储
   - 支持增量更新

## 输出数据格式

爬取的数据将保存在 `data/raw` 目录下，采用 CSV 格式。

### CSV 文件结构
文件名：`thesis_titles_{year}.csv`

| 列名 | 类型 | 说明 | 示例 |
|-----|------|------|------|
| id | int | 论文唯一标识符 | 1 |
| title | string | 论文标题（已清洗） | 基于深度学习的文本分类研究 |
| publish_date | string | 发表日期 | 2024-12-30 |
| author | string | 第一作者 | TomyJan |
| major | string | 专业分类 | 信息科技 |

### 数据要求
- 编码：UTF-8
- 分隔符：逗号 (,)
- 包含表头
- 所有字段都不允许为空
- 标题必须是规范的中文或英文，不包含特殊字符
- 日期格式为 YYYY-MM-DD

## 配置说明

在 `src/config.py` 中设置爬虫参数：

```python
CRAWLER_CONFIG = {
    "retry_times": 3,        # 重试次数
    "timeout": 10,           # 请求超时时间
    "delay": [1, 3],        # 随机延时范围
    "batch_size": 100       # 批处理大小
}
```

## 使用示例

```python
from src.crawler.crawler import Crawler

# 初始化爬虫
crawler = Crawler()

# 爬取单个年份的数据
crawler.crawl_year(2024)

# 批量爬取多个年份
years = [2020, 2021, 2022, 2023, 2024]
crawler.crawl_years(years)
```

## 注意事项

1. 遵守网站的爬虫协议
2. 适当设置请求延时，避免被封IP
3. 定期检查数据完整性
4. 及时处理和记录异常情况
5. 注意磁盘空间使用情况
